{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# This code will run all the functions from cleaning the data, processing it and then outputting the results\n",
    "\n",
    "The directories may need to be adjusted as the code does not output all at once\n",
    "- Due to short timeframes the code may not run as efficently. I have inputted this code from the various files I used throughout, so code may repeat itself.\n",
    "- These were individual scripts that I used aggregated into this notebook\n",
    "- There may be conficts within the code that might need to be resolved "
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "First we pull the main functions into python"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import pandas as pd\n",
    "import yfinance as yf \n",
    "import numpy as np\n",
    "#Need to add for different tickers\n",
    "\n",
    "#ticker_list = ['QQQ', 'AMC', 'MSFT', 'ARKK', 'GME', 'SPY'] # AMZN\n",
    "ticker_list = ['NVDA', 'XOM', 'AAPL', 'NFLX', 'TSLA', 'AMD','QQQ', 'MSFT', 'ARKK', 'GME', 'SPY', 'AMZN']\n",
    "#df = pd.read_csv('../spy_2020.csv', index_col=0)\n",
    "\n",
    "\n",
    "def merge_df(df, data):\n",
    "\n",
    "    df['date']= pd.to_datetime(df['date'], format='%Y%m%d', errors='ignore')\n",
    "\n",
    "\n",
    "    #resets the index, you cant just change names of indexes\n",
    "    data.reset_index(inplace=True)\n",
    "    data=data.rename(columns= {'Date': 'date'})\n",
    "\n",
    "    data['Close'] = data['Close'].round(2) #rounding the price on close\n",
    "    data = data[['date','Close', 'Adj Close']] #Pull only the date and the close \n",
    "    df = df.merge(data, on='date') #merge data onto the df using date\n",
    "\n",
    "    #df['Close'].loc[((df['date'] == 'C')] \n",
    "    return df\n",
    "\n",
    "\n",
    "#function to get date difference between epiry and the current date\n",
    "def get_t(df_loc):\n",
    "    exp_dates_convert = pd.to_datetime(df_loc['exdate'], format='%Y%M%d', errors='ignore')\n",
    "    current_date_convert = pd.to_datetime(df_loc['date'],format='%Y-%M-%d', errors='ignore')\n",
    "    days_to_expiry = exp_dates_convert - current_date_convert\n",
    "    return (days_to_expiry.dt.days/365)\n",
    "\n",
    "\n",
    "\n",
    "from datetime import datetime as dt\n",
    "import time\n",
    "\n",
    "    "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "We clean the data - much of the values were deep ITM options that weren't traded anymore"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    " \n",
    "\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "for i in ticker_list:\n",
    "    loop_time = time.time()\n",
    "    df = pd.read_csv(f'../{i}.csv')\n",
    "    df['Intrinsic_Value'] = np.nan\n",
    "    df['strike_price'] = df['strike_price']/1000\n",
    "    \n",
    "    final_day=df.date.head(1).item()\n",
    "    final_day = str(final_day)\n",
    "    date_object = dt.strptime(final_day, '%Y%M%d')\n",
    "    final_day= dt.strftime(date_object, '%Y-%M-%d')\n",
    "\n",
    "    data = yf.download(f\"{i}\", start=final_day, end=\"2021-01-01\")\n",
    "    df = merge_df(df, data)\n",
    "\n",
    "    df['Intrinsic_Value'].loc[df['cp_flag'] == 'C'] = (df['Close']-df['strike_price']) \n",
    "    df['Intrinsic_Value'].loc[df['cp_flag'] == 'P'] = (df['strike_price']-df['Close']) \n",
    "\n",
    "\n",
    "\n",
    "    df['impl_volatility'].loc[(df['impl_volatility'].isnull())] = 0\n",
    "    \n",
    "    df['delta'].loc[df['delta'].isnull()] = 1\n",
    "    df['gamma'].loc[df['gamma'].isnull()] = 0\n",
    "\n",
    "    #if the gamma is NaN, if sigma=0 as the entire equation is divided by it then we just set it to 0\n",
    "\n",
    "    df['delta'].loc[(df['delta']) == 0 & (df['cp_flag'] == 'P')] = 1\n",
    "\n",
    "\n",
    "\n",
    "    df['theta'].loc[(df['theta'].isnull()) & (df['cp_flag'] == 'C')] = 0\n",
    "    df['theta'].loc[df['theta'].isnull() & df['cp_flag'] == 'P' ] = 0\n",
    "\n",
    "\n",
    "    df['vega'].loc[df['vega'].isnull()] = 0\n",
    "\n",
    "    df.to_csv(f'{i}_edit_2.csv')\n",
    "\n",
    "    del df\n",
    "    loop_time_end = time.time()\n",
    "    print(loop_time-loop_time_end)\n",
    "\n",
    "end = time.time()\n",
    "\n",
    "print(f'final time is {end - start}')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "We add the higer order greeks"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import pandas as pd\n",
    "import numpy as np \n",
    "#from math import sqrt, pi,log, e\n",
    "from enum import Enum\n",
    "import scipy.stats as stat\n",
    "from scipy.stats import norm\n",
    "\n",
    "\n",
    "def get_t(df):\n",
    "    exp_dates_convert = pd.to_datetime(df['exdate'], format='%Y%m%d', errors='ignore')\n",
    "\n",
    "    current_date_convert = pd.to_datetime(df['date'],format='%d/%M/%Y', errors='ignore')\n",
    "\n",
    "    print(exp_dates_convert.dtypes)\n",
    "    print(current_date_convert.dtypes)\n",
    "\n",
    "    print(exp_dates_convert)\n",
    "    print(current_date_convert)\n",
    "    exp_dates_convert  = exp_dates_convert.astype('datetime64[ns]')\n",
    "    current_date_convert  = current_date_convert.astype('datetime64[ns]')\n",
    "    days_to_expiry = exp_dates_convert - current_date_convert\n",
    "    return (days_to_expiry.dt.days/365)\n",
    "\n",
    "\n",
    "\n",
    "def zomma():\n",
    "    d1 = (np.log(S/K)+(r+v*v/2.)*t)/(v*np.sqrt(t))\n",
    "    d1 = np.nan_to_num(d1)\n",
    "    d2 = d1-v*np.sqrt(t)\n",
    "    zomma = np.exp(t) * (norm.pdf(d1)*(d1*d2-1)) / (S * v**2 * np.sqrt(t))\n",
    "    return zomma\n",
    "\n",
    "def vanna():\n",
    "    d1 = 1 / (v * np.sqrt(t)) * ( np.log(S/K) + (r + v**2/2) * t)\n",
    "    d2 = d1 - v * np.sqrt(t)\n",
    "    vanna = -np.exp(t)* (norm.pdf(d1)*((d2)/v))\n",
    "    return vanna\n",
    "\n",
    "\n",
    "ticker_list = ['NVDA', 'XOM', 'AAPL', 'NFLX', 'TSLA', 'AMD','QQQ', 'MSFT', 'ARKK', 'GME', 'SPY', 'AMZN']\n",
    "for i in ticker_list:\n",
    "    \n",
    "    df = pd.read_csv(f'../{i}_edit_2.csv')\n",
    "    S = df['Close']\n",
    "    r=0.1\n",
    "    q=0\n",
    "    K= df['strike_price']\n",
    "    v = df['impl_volatility']\n",
    "    t = get_t(df)\n",
    "    print(zomma())\n",
    "\n",
    "    df['zomma'] = zomma()\n",
    "    df['vanna'] = vanna()\n",
    "\n",
    "    df.to_csv(f'../{i}_added_greeks.csv')\n",
    "    del df"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "We calculate the assoicated models"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "from datetime import date, timedelta\n",
    "import matplotlib.pyplot as plt \n",
    "from datetime import datetime as dt\n",
    "import yfinance as yf\n",
    "import time \n",
    "\n",
    "\n",
    "\n",
    "def merge_df(df, data):\n",
    "\n",
    "    df['date']= pd.to_datetime(df['date'], format='%Y-%m-%d', errors='ignore')\n",
    "    #print(df.date[1])\n",
    "\n",
    "    #resets the index, you cant just change names of indexes\n",
    "    data.reset_index(inplace=True)\n",
    "    data=data.rename(columns= {'Date': 'date'})\n",
    "    \n",
    "    data['underling_volume'] = data['Volume'].round(2) #rounding the price on close\n",
    "    data = data[['date','underling_volume']]\n",
    "    \n",
    "     #Pull only the date and the close \n",
    "    df = df.merge(data, on='date') #merge data onto the df using date\n",
    "    return df\n",
    "\n",
    "\n",
    "ticker_list = ['NVDA', 'XOM', 'AAPL', 'NFLX', 'TSLA', 'AMD','QQQ', 'MSFT', 'ARKK', 'GME', 'SPY', 'AMZN']\n",
    "for i in ticker_list:\n",
    "\n",
    "    \n",
    "        \n",
    "\n",
    "    new_df = pd.read_csv(f'../{i}_added_greeks.csv', usecols=['date', 'cp_flag',  'delta', 'gamma', 'volume', 'open_interest', 'impl_volatility', 'vanna', 'zomma'])\n",
    "    new_df = new_df.loc[(new_df['volume'].notnull()) & (new_df['open_interest']>0) & (new_df['volume']>0)]\n",
    "    new_df = new_df.fillna(0)\n",
    "    print(new_df)\n",
    "\n",
    "    final_day=new_df.date.head(1).item()\n",
    "    df_len = len(pd.date_range(final_day, end='1-1-2021', freq= 'B'))\n",
    "\n",
    "\n",
    "    print(final_day)\n",
    "    final_day = dt.strptime(final_day, '%Y-%m-%d')\n",
    "    day_delta=[]\n",
    "    gex_day =[]\n",
    "    date_list=[]\n",
    "    call_volume_aggregate_list=[]\n",
    "    put_volume_aggregate_list = []\n",
    "    call_open_interest_aggregate_list = []\n",
    "    put_open_interest_aggregate_list = []\n",
    "    call_iv_aggregate_list = []\n",
    "    put_iv_aggregate_list=[]\n",
    "    nope_oi_list =[]\n",
    "    gex_volume_list=[]\n",
    "    net_vanna_oi_list = []\n",
    "    net_vanna_volume_list =[]\n",
    "    net_zomma_oi_list =[]\n",
    "    net_zomma_volume_list =[] \n",
    "    day_delta_dict = {}\n",
    "\n",
    "\n",
    "    data = yf.download(f\"{i}\", start=final_day, end=\"2021-01-01\")\n",
    "\n",
    "    snp_return = data['Close'] - data['Close'].shift()\n",
    "\n",
    "    snp_return_percentage = ((data['Close'] - data['Close'].shift())/data['Close'].shift())*100 \n",
    "    print(data['Close'])\n",
    "    print(snp_return)\n",
    "    print(snp_return_percentage)\n",
    "\n",
    "    k=0\n",
    "    printcounter = 0\n",
    "    n_time = time.time()\n",
    "    time_holder = time.time()\n",
    "    #d = {'date' : np.nan, 'delta': np.nan, 'NOPE': np.nan}\n",
    "    df_test = pd.DataFrame(columns= ['date', 'delta'])\n",
    "    \n",
    "\n",
    "\n",
    "    #EDIT THIS SO THAT IT CHANGES TO A VECTOR SOLUTION AND NOT A LOOP, maybe look at iterrows \n",
    "    for j in pd.date_range(final_day, end='1-1-2021', freq='B'):\n",
    "        \n",
    "\n",
    "        #print(j)\n",
    "        df_loc = new_df[(new_df['cp_flag']=='C') & (new_df['date']==j)] # ==j =new_df['date'][k])]\n",
    "        df_put = new_df[(new_df['cp_flag']=='P') & (new_df['date']==j)] # MAYBE ADD IN WHERE VOLUME IS BIGGER THAN 0 AS WE DONT LOOK AT THAT FOR NOW \n",
    "\n",
    "        df_test['date'] = j  \n",
    "\n",
    "\n",
    "        nope = ((df_loc['volume']*df_loc['delta']).sum()+(df_put['volume']*df_put['delta']).sum())   \n",
    "        nope_oi =  ((df_loc['open_interest']*df_loc['delta']).sum()+(df_put['open_interest']*df_put['delta']).sum()) \n",
    "\n",
    "\n",
    "        gex = ((df_loc['open_interest']*df_loc['gamma']).sum()-(df_put['open_interest']*df_put['gamma']).sum())\n",
    "        gex_volume = ((df_loc['volume']*df_loc['gamma']).sum()-(df_put['volume']*df_put['gamma']).sum())\n",
    "\n",
    "\n",
    "        net_vanna_oi =  ((df_loc['open_interest']*df_loc['vanna']).sum()+(df_put['open_interest']*df_put['vanna']).sum())\n",
    "        net_vanna_volume =  ((df_loc['volume']*df_loc['vanna']).sum()+(df_put['volume']*df_put['vanna']).sum())\n",
    "\n",
    "        net_zomma_oi =  ((df_loc['open_interest']*df_loc['zomma']).sum()+(df_put['open_interest']*df_put['zomma']).sum())\n",
    "        net_zomma_volume =  ((df_loc['volume']*df_loc['zomma']).sum()+(df_put['volume']*df_put['zomma']).sum())\n",
    "\n",
    "\n",
    "        call_volume_aggregate = df_loc['volume'].sum()\n",
    "        put_volume_aggregate = df_put['volume'].sum()\n",
    "\n",
    "        call_open_interest_aggregate = df_loc['open_interest'].sum()\n",
    "        put_open_interest_aggregate = df_put['open_interest'].sum()\n",
    "\n",
    "\n",
    "        call_iv_aggregate = (df_loc['open_interest']*df_loc['impl_volatility']).mean()\n",
    "        put_iv_aggregate = (df_put['open_interest']*df_put['impl_volatility']).mean()\n",
    "        delta_holder = nope\n",
    "\n",
    "\n",
    "        gex_day.append(gex)\n",
    "        day_delta.append(delta_holder)\n",
    "        date_list.append(j)\n",
    "        call_volume_aggregate_list.append(call_volume_aggregate)\n",
    "        put_volume_aggregate_list.append(put_volume_aggregate)\n",
    "        call_open_interest_aggregate_list.append(call_open_interest_aggregate)\n",
    "        put_open_interest_aggregate_list.append(put_open_interest_aggregate)\n",
    "        call_iv_aggregate_list.append(call_iv_aggregate)\n",
    "        put_iv_aggregate_list.append(put_iv_aggregate)\n",
    "\n",
    "        nope_oi_list.append(nope_oi)\n",
    "        gex_volume_list.append(gex_volume)\n",
    "        net_vanna_oi_list.append(net_vanna_oi)\n",
    "        net_vanna_volume_list.append(net_vanna_volume)\n",
    "        net_zomma_oi_list.append(net_zomma_oi)\n",
    "        net_zomma_volume_list.append(net_zomma_volume)\n",
    "\n",
    "\n",
    "\n",
    "        if (k % 100 == 0) and k>0:\n",
    "            \n",
    "            old_time = time.time()\n",
    "\n",
    "            print(f'number {k}'); print(f'{(old_time - n_time)/60} mins total'); print(f'{round((k/df_len)*100, 3)}%')\n",
    "            print(df_len, k)\n",
    "            print((df_len-k)/100)\n",
    "            print(f'estimated time : {((old_time-time_holder)*(df_len-k)/100)/60} mins') #df_len/k)-k\n",
    "\n",
    "            print(f'{k} out of {df_len} days')\n",
    "            print('========='*6)\n",
    "\n",
    "            time_holder = time.time()\n",
    "\n",
    "\n",
    "\n",
    "        del df_loc\n",
    "        del df_put\n",
    "        k=k+1\n",
    "\n",
    "    day_delta_dict['delta'] = day_delta\n",
    "    day_delta_dict['gex'] = gex_day\n",
    "    day_delta_dict['date'] = date_list\n",
    "    day_delta_dict['call_volume_aggregate_list'] = call_volume_aggregate_list\n",
    "    day_delta_dict['put_volume_aggregate_list'] = put_volume_aggregate_list\n",
    "    day_delta_dict['call_open_interest_aggregate'] = call_open_interest_aggregate_list\n",
    "    day_delta_dict['put_open_interest_aggregate'] = put_open_interest_aggregate_list\n",
    "    day_delta_dict['call_iv_aggregate'] = call_iv_aggregate_list\n",
    "    day_delta_dict['put_iv_aggregate']= put_iv_aggregate_list\n",
    "\n",
    "    day_delta_dict['nope_oi']= nope_oi_list\n",
    "    day_delta_dict['gex_volume']= gex_volume_list\n",
    "    day_delta_dict['net_vanna_oi']= net_vanna_oi_list\n",
    "    day_delta_dict['net_vanna_volume']= net_vanna_volume_list\n",
    "    day_delta_dict['net_zomma_oi']= net_zomma_oi_list\n",
    "    day_delta_dict['net_zomma_volume']= net_zomma_volume_list\n",
    "\n",
    "    df_final = pd.DataFrame(day_delta_dict)\n",
    "    \n",
    "    df_final = merge_df(df_final, data)\n",
    "\n",
    "\n",
    "    df_final['nope'] = df_final['delta'] / df_final['underling_volume'] \n",
    "\n",
    "    df_final['nope_oi_normalised'] = df_final['nope_oi'] / df_final['underling_volume'] \n",
    "    df_final['gex_volume_normalised'] = df_final['gex_volume'] / df_final['underling_volume']\n",
    "    df_final['gex_normalised'] = df_final['gex'] / df_final['underling_volume'] \n",
    "    df_final['net_vanna_oi_normalised']= df_final['net_vanna_oi'] / df_final['underling_volume'] \n",
    "    df_final['net_vanna_volume_normalised']= df_final['net_vanna_volume'] / df_final['underling_volume'] \n",
    "    df_final['net_zomma_oi_normalised']= df_final['net_zomma_oi']/ df_final['underling_volume'] \n",
    "    df_final['net_zomma_volume_normalised']= df_final['net_zomma_volume']/ df_final['underling_volume'] \n",
    "\n",
    "    print('=================')\n",
    "    print(df_final)\n",
    "    print('=================')\n",
    "\n",
    "\n",
    "    df_final.to_csv(f'../{i}nope_3.csv')\n",
    "    day_delta.clear()\n",
    "    gex_day.clear()\n",
    "    call_volume_aggregate_list.clear()\n",
    "    put_volume_aggregate_list.clear()\n",
    "    call_open_interest_aggregate_list.clear()\n",
    "    put_open_interest_aggregate_list.clear() \n",
    "    call_iv_aggregate_list.clear()\n",
    "    put_iv_aggregate_list.clear()\n",
    "\n",
    "    nope_oi_list.clear()\n",
    "    gex_volume_list.clear()\n",
    "    net_vanna_oi_list.clear()\n",
    "    net_vanna_volume_list.clear()\n",
    "    net_zomma_oi_list.clear()\n",
    "    net_zomma_volume_list.clear()\n",
    "\n",
    "    del df_final\n",
    "    del day_delta_dict"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "We get the regression values"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import pandas as pd \n",
    "import yfinance as yf\n",
    "from datetime import datetime as dt\n",
    "import statsmodels.api as sm\n",
    "import numpy as np\n",
    "\n",
    "def merge_df(df, data):\n",
    "\n",
    "    df['date']= pd.to_datetime(df['date'], format='%Y-%m-%d', errors='ignore')\n",
    "    #print(df.date[1])\n",
    "\n",
    "    #resets the index, you cant just change names of indexes\n",
    "    data.reset_index(inplace=True)\n",
    "    data=data.rename(columns= {'Date': 'date'})\n",
    "    \n",
    "    data['close'] = data['Close'].round(2) #rounding the price on close\n",
    "    data = data[['date','close', 'daily_return', 'stock_return_percentage']]\n",
    "    \n",
    "     #Pull only the date and the close \n",
    "    df = df.merge(data, on='date') #merge data onto the df using date\n",
    "\n",
    "    #df['Close'].loc[((df['date'] == 'C')] \n",
    "    return df\n",
    "\n",
    "\n",
    "\n",
    "tickers = ['MSFT', 'NVDA', 'XOM', 'AAPL', 'NFLX', 'TSLA', 'AMD']\n",
    "\n",
    "#These are the additional regressors tested, I only included the main ones, however some of the models showed promising results, as I alluded to in the main dissertation\n",
    "#regresors = ['nope', 'gex', 'gex_positive', 'gex_negative', 'nope_oi', 'nope_oi_normalised', 'gex_volume_normalised', 'gex_normalised', 'net_vanna_oi',\t'net_vanna_volume',\t'net_zomma_oi',\t'net_zomma_volume',\t'net_vanna_oi_normalised',\t'net_vanna_volume_normalised',\t'net_zomma_oi_normalised', 'net_zomma_volume_normalised']\n",
    "\n",
    "\n",
    "regresors = ['nope', 'gex', 'gex_positive', 'gex_negative', 'nope_oi','net_vanna_oi','net_zomma_oi']\n",
    "\n",
    "\n",
    "\n",
    "regression_dict = {}\n",
    "regression_dict['regresors'] = regresors\n",
    "paramvalues_list = []\n",
    "p_values_list = []\n",
    "r_squared_list =[]\n",
    "intercept_list=[]\n",
    "\n",
    "for ticker in tickers:\n",
    "  #  print(ticker)\n",
    "    df = pd.read_csv(f'../{ticker}nope_3.csv')\n",
    "    \n",
    "# print(df)\n",
    "    data = yf.download(ticker)\n",
    "    \n",
    "    dividor = 100000\n",
    "    df['nope'] = df['nope']\n",
    "    df['gex'] = df['gex']/dividor\n",
    "\n",
    "    df['nope_oi'] = df['nope_oi']/dividor\n",
    "    df['net_vanna_oi'] = df['net_vanna_oi']/dividor\n",
    "    df['net_zomma_oi'] = df['net_zomma_oi']/dividor\n",
    "\n",
    "    df['nope'] = df['nope']*100\n",
    "    df['gex'] = df['gex']*100\n",
    "    df['gex_positive'] = df['gex'].loc[(df['gex']>0)]\n",
    "    df['gex_negative'] = df['gex'].loc[(df['gex']<0)]\n",
    "    final_day=df.date.head(1).item()\n",
    "    final_day = dt.strptime(final_day, '%Y-%m-%d')\n",
    "\n",
    "    data = yf.download(ticker, start=final_day, end=\"2021-01-01\")\n",
    "\n",
    "    stock_return = data['Close'] - data['Close'].shift()\n",
    "\n",
    "    data['daily_return'] = stock_return\n",
    "    stock_return_percentage = ((data['Close'] - data['Close'].shift())/data['Close'].shift())*100\n",
    "    data['stock_return_percentage'] = stock_return_percentage\n",
    "\n",
    "\n",
    "    df = merge_df(df, data)\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "    \n",
    "    import statsmodels.api as sm\n",
    "\n",
    "    df = df.set_index('date')\n",
    "    start_year = 2015\n",
    "    end_year = 2020\n",
    "    df = df[f'{start_year}0101':f'{end_year}1230']\n",
    "    df = df.fillna(0)\n",
    "    df = df.replace([np.inf, -np.inf], 0)\n",
    "\n",
    "\n",
    "    for j in regresors:\n",
    "\n",
    "\n",
    "        regressor_var = j\n",
    "    \n",
    "    \n",
    "        \n",
    "       \n",
    "        #print(ticker)\n",
    "    # df = df.replace([np.inf, -np.inf], 0, inplace=True)\n",
    "      #  print(df[j])\n",
    "        X = df[j]\n",
    "        y = df.stock_return_percentage.shift(fill_value=0)\n",
    "\n",
    "        \n",
    "\n",
    "        #X = np.array(X)\n",
    "        #y = np.array(y)\n",
    "        y= list(y)\n",
    "        X= list(X)\n",
    "        X = sm.add_constant(X)\n",
    "        #print(pd.isNan(X))\n",
    "        #print(y)\n",
    "        #X = sm.add_constant(X)\n",
    "        ols_resid = sm.OLS(y, X).fit().resid\n",
    "        res_fit = sm.OLS(ols_resid[1:], ols_resid[:-1]).fit()\n",
    "        rho = res_fit.params\n",
    "        \n",
    "        from scipy.linalg import toeplitz\n",
    "\n",
    "        \n",
    "        order = toeplitz(range(len(ols_resid)))\n",
    "        sigma = rho**order\n",
    "        gls_model = sm.GLS(y, X, sigma=sigma)\n",
    "        gls_results = gls_model.fit()\n",
    "        print(j)\n",
    "        print(gls_results.summary())\n",
    "\n",
    "        #gls_results = sm.OLS(y, X).fit()\n",
    "        #print(gls_results.summary())\n",
    "        #print(gls_results.params)\n",
    "       # print(gls_results.rsquared_adj)\n",
    "        #print(gls_results.pvalues)\n",
    "        gls_results.params[0] = gls_results.params[0]*100\n",
    "        gls_results.params[0] = round(gls_results.params[0], 5) \n",
    "        gls_results.params[1] = round(gls_results.params[1], 5) \n",
    "        gls_results.pvalues[0] = round(gls_results.pvalues[0], 5)\n",
    "        param_result_string = str(gls_results.params)#[0])\n",
    "        #gls_rsquared = gls_results.rsquared[0]\n",
    "\n",
    "        if gls_results.pvalues[0] < 0.1 and gls_results.pvalues[0] > 0.05:\n",
    "            new_intercept_val = f'{str(gls_results.params[0])}*  ({round(gls_results.tvalues[0], 3)})'\n",
    "        if gls_results.pvalues[0] < 0.05 and gls_results.pvalues[0] > 0.01:\n",
    "            new_intercept_val = f'{str(gls_results.params[0])}**  ({round(gls_results.tvalues[0], 3)})'\n",
    "        if gls_results.pvalues[0] < 0.01:\n",
    "            new_intercept_val = f'{str(gls_results.params[0])}***  ({round(gls_results.tvalues[0], 3)})'\n",
    "        else: \n",
    "            new_intercept_val = f'{str(gls_results.params[0])} ({round(gls_results.tvalues[0], 3)})'\n",
    "\n",
    "        if gls_results.pvalues[1] < 0.1 and gls_results.pvalues[1] > 0.05:\n",
    "            new_beta_val = f'{str(gls_results.params[1])}e-5*  ({round(gls_results.tvalues[1], 3)})'\n",
    "        if gls_results.pvalues[1] < 0.05 and gls_results.pvalues[1] > 0.01:\n",
    "            new_beta_val = f'{str(gls_results.params[1])}e-5**  ({round(gls_results.tvalues[1], 3)})'\n",
    "        if gls_results.pvalues[1] < 0.01:\n",
    "            new_beta_val = f'{str(gls_results.params[1])}e-5*** ({round(gls_results.tvalues[1], 3)})'\n",
    "        else: \n",
    "            new_beta_val = f'{str(gls_results.params[1])}e-5 ({round(gls_results.tvalues[1], 3)})'\n",
    "\n",
    "        '''\n",
    "        if 'Regressor' in regression_dict:\n",
    "            regression_dict['regressor'].append(regressor_var)\n",
    "        else:\n",
    "            regression_dict['regressor'] = regressor_var\n",
    "        '''\n",
    "\n",
    "        #if f'{ticker}_pararm' in regression_dict:\n",
    "        param_result_string = str(gls_results.params[0])\n",
    "        #print(param_result_string)\n",
    "        intercept_list.append(new_intercept_val)\n",
    "        paramvalues_list.append(new_beta_val)\n",
    "        r_squared_list.append(round(gls_results.rsquared_adj*100, 3))\n",
    "\n",
    "        #p_values_list.append(new_p_val)\n",
    "\n",
    "    param_holder = paramvalues_list\n",
    "    p_holder = p_values_list\n",
    "\n",
    "    r_holder = r_squared_list\n",
    "\n",
    "    regression_dict[f'{ticker} Intercept'] = intercept_list\n",
    "    regression_dict[f'{ticker} beta t'] = param_holder\n",
    "    regression_dict[f'{ticker} R Squared %'] = r_holder\n",
    "        #change this to the code on top after\n",
    "   # regression_dict[f'{ticker}_pvalues'] = p_holder\n",
    "    \n",
    "    '''\n",
    "    paramvalues_list.clear()\n",
    "    p_values_list.clear()\n",
    "\n",
    "    '''\n",
    "    #print(f\"{ticker}, {len(regression_dict[f'{ticker}_pararm'])}\")\n",
    "        #store them in a list then a dict, then save them to csv, then add them into word or whatever\n",
    "        #add multiple values\n",
    "\n",
    "\n",
    "    del df\n",
    "  #  print(regression_dict)\n",
    "    #del paramvalues_list\n",
    "    #del p_values_list\n",
    "    \n",
    "   # print(len(paramvalues_list))\n",
    "   # print(len(intercept_list))\n",
    "  #  print(len(param_holder[0]))\n",
    "   # print(len(param_holder[1]))\n",
    "   # print(len(p_values_list))\n",
    "   # print(len(r_squared_list))\n",
    "    intercept_list =[]\n",
    "    paramvalues_list = []\n",
    "    p_values_list = []\n",
    "    r_squared_list = []\n",
    "\n",
    "    \n",
    "print(regression_dict)\n",
    "print(len(regression_dict))\n",
    "df_final = pd.DataFrame(regression_dict)\n",
    "df_final.to_csv('single_stock_regression_3.csv')\n",
    "\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "For GME"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import pandas as pd \n",
    "import yfinance as yf\n",
    "from datetime import datetime as dt\n",
    "import statsmodels.api as sm\n",
    "import numpy as np\n",
    "\n",
    "def merge_df(df, data):\n",
    "\n",
    "    df['date']= pd.to_datetime(df['date'], format='%Y-%m-%d', errors='ignore')\n",
    "    #print(df.date[1])\n",
    "\n",
    "    #resets the index, you cant just change names of indexes\n",
    "    data.reset_index(inplace=True)\n",
    "    data=data.rename(columns= {'Date': 'date'})\n",
    "    \n",
    "    data['close'] = data['Close'].round(2) #rounding the price on close\n",
    "    data = data[['date','close', 'daily_return', 'stock_return_percentage']]\n",
    "    \n",
    "     #Pull only the date and the close \n",
    "    df = df.merge(data, on='date') #merge data onto the df using date\n",
    "    return df\n",
    "\n",
    "\n",
    "\n",
    "tickers = ['GME']\n",
    "#regresors = ['nope', 'gex', 'gex_positive', 'gex_negative', 'nope_oi', 'nope_oi_normalised', 'gex_volume_normalised', 'gex_normalised', 'net_vanna_oi',\t'net_vanna_volume',\t'net_zomma_oi',\t'net_zomma_volume',\t'net_vanna_oi_normalised',\t'net_vanna_volume_normalised',\t'net_zomma_oi_normalised', 'net_zomma_volume_normalised']\n",
    "regresors = ['nope', 'gex', 'gex_positive', 'gex_negative', 'nope_oi','net_vanna_oi','net_zomma_oi']\n",
    "\n",
    "\n",
    "\n",
    "regression_dict = {}\n",
    "regression_dict['regresors'] = regresors\n",
    "paramvalues_list = []\n",
    "p_values_list = []\n",
    "r_squared_list =[]\n",
    "intercept_list=[]\n",
    "\n",
    "for ticker in tickers:\n",
    "    df = pd.read_csv(f'../{ticker}nope_3.csv')\n",
    "\n",
    "    data = yf.download(ticker)\n",
    "    \n",
    "    \n",
    "    dividor = 100000\n",
    "    df['nope'] = df['nope']\n",
    "    df['gex'] = df['gex']/dividor\n",
    "\n",
    "    df['nope_oi'] = df['nope_oi']/dividor\n",
    "    df['net_vanna_oi'] = df['net_vanna_oi']/dividor\n",
    "    df['net_zomma_oi'] = df['net_zomma_oi']/dividor\n",
    "    df['gex_positive'] = df['gex'].loc[(df['gex']>0)]\n",
    "    df['gex_negative'] = df['gex'].loc[(df['gex']<0)]\n",
    "    final_day=df.date.head(1).item()\n",
    "    final_day = dt.strptime(final_day, '%Y-%m-%d')\n",
    "\n",
    "    data = yf.download(ticker, start=final_day, end=\"2021-01-01\")\n",
    "\n",
    "    stock_return = data['Close'] - data['Close'].shift()\n",
    "\n",
    "    data['daily_return'] = stock_return\n",
    "    stock_return_percentage = ((data['Close'] - data['Close'].shift())/data['Close'].shift())*100\n",
    "    data['stock_return_percentage'] = stock_return_percentage\n",
    "\n",
    "\n",
    "    df = merge_df(df, data)\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "    \n",
    "    import statsmodels.api as sm\n",
    "\n",
    "#nope_oi_normalised\tgex_volume_normalised\tgex_normalised\tnet_vanna_oi_normalised\tnet_vanna_volume_normalised\tnet_zomma_oi_normalised\tnet_zomma_volume_normalised\n",
    "    df = df.set_index('date')\n",
    "    start_year = 2020\n",
    "    end_year = 2020\n",
    "    df = df[f'{start_year}0101':f'{end_year}1230']\n",
    "    df = df.fillna(0)\n",
    "    df = df.replace([np.inf, -np.inf], 0)\n",
    "\n",
    "\n",
    "    for j in regresors:\n",
    "\n",
    "\n",
    "        regressor_var = j\n",
    "        X = df[j]\n",
    "        y = df.stock_return_percentage.shift(fill_value=0)\n",
    "\n",
    "        y= list(y)\n",
    "        X= list(X)\n",
    "        X = sm.add_constant(X)\n",
    "\n",
    "        ols_resid = sm.OLS(y, X).fit().resid\n",
    "        res_fit = sm.OLS(ols_resid[1:], ols_resid[:-1]).fit()\n",
    "        rho = res_fit.params\n",
    "        \n",
    "        from scipy.linalg import toeplitz\n",
    "\n",
    "        \n",
    "        order = toeplitz(range(len(ols_resid)))\n",
    "        sigma = rho**order\n",
    "        gls_model = sm.GLS(y, X, sigma=sigma)\n",
    "        gls_results = gls_model.fit()\n",
    "        print(gls_results.rsquared_adj)\n",
    "        #print(gls_results.pvalues)\n",
    "        gls_results.params[0] = gls_results.params[0]*100\n",
    "        gls_results.params[0] = round(gls_results.params[0], 5) \n",
    "        gls_results.pvalues[0] = round(gls_results.pvalues[0], 5)\n",
    "        param_result_string = str(gls_results.params)#[0])\n",
    "        #gls_rsquared = gls_results.rsquared[0]\n",
    "\n",
    "        if gls_results.pvalues[0] < 0.1 and gls_results.pvalues[0] > 0.05:\n",
    "            new_intercept_val = f'{str(gls_results.params[0])}*  ({round(gls_results.tvalues[0], 3)})'\n",
    "        if gls_results.pvalues[0] < 0.05 and gls_results.pvalues[0] > 0.01:\n",
    "            new_intercept_val = f'{str(gls_results.params[0])}**  ({round(gls_results.tvalues[0], 3)})'\n",
    "        if gls_results.pvalues[0] < 0.01:\n",
    "            new_intercept_val = f'{str(gls_results.params[0])}***  ({round(gls_results.tvalues[0], 3)})'\n",
    "        else: \n",
    "            new_intercept_val = f'{str(gls_results.params[0])} ({round(gls_results.tvalues[0], 3)})'\n",
    "\n",
    "        if gls_results.pvalues[1] < 0.1 and gls_results.pvalues[1] > 0.05:\n",
    "            new_beta_val = f'{str(gls_results.params[1])}e-5*  ({round(gls_results.tvalues[1], 3)})'\n",
    "        if gls_results.pvalues[1] < 0.05 and gls_results.pvalues[1] > 0.01:\n",
    "            new_beta_val = f'{str(gls_results.params[1])}e-5**  ({round(gls_results.tvalues[1], 3)})'\n",
    "        if gls_results.pvalues[1] < 0.01:\n",
    "            new_beta_val = f'{str(gls_results.params[1])}e-5*** ({round(gls_results.tvalues[1], 3)})'\n",
    "        else: \n",
    "            new_beta_val = f'{str(gls_results.params[1])}e-5 ({round(gls_results.tvalues[1], 3)})'\n",
    "\n",
    "        '''\n",
    "        if 'Regressor' in regression_dict:\n",
    "            regression_dict['regressor'].append(regressor_var)\n",
    "        else:\n",
    "            regression_dict['regressor'] = regressor_var\n",
    "        '''\n",
    "\n",
    "        #if f'{ticker}_pararm' in regression_dict:\n",
    "        param_result_string = str(gls_results.params[0])\n",
    "        #print(param_result_string)\n",
    "        intercept_list.append(new_intercept_val)\n",
    "        paramvalues_list.append(new_beta_val)\n",
    "        r_squared_list.append(round(gls_results.rsquared_adj*100, 3))\n",
    "\n",
    "        #p_values_list.append(new_p_val)\n",
    "\n",
    "    param_holder = paramvalues_list\n",
    "    p_holder = p_values_list\n",
    "\n",
    "    r_holder = r_squared_list\n",
    "\n",
    "    regression_dict[f'{ticker} Intercept'] = intercept_list\n",
    "    regression_dict[f'{ticker} beta t'] = param_holder\n",
    "    regression_dict[f'{ticker} R Squared %'] = r_holder\n",
    "\n",
    "\n",
    "    del df\n",
    "    print(regression_dict)\n",
    "    intercept_list =[]\n",
    "    paramvalues_list = []\n",
    "    p_values_list = []\n",
    "    r_squared_list = []\n",
    "\n",
    "    \n",
    "print(regression_dict)\n",
    "print(len(regression_dict))\n",
    "df_final = pd.DataFrame(regression_dict)\n",
    "df_final.to_csv('GME_regression_values_3.csv')\n",
    "\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}